mtext(expression("Onset\nWeek"),side=2,las=1,line=1,cex=0.75,padj=1)
dev.off()
}
setwd(paste("~/Github/1718Flu/WP_plots/",paste("Report",report,sep=""),sep=""))
for(h in 1:10){
today <- get_flu_data("hhs",h,"ilinet",2017)
onset_sim <- peaksize_sim <- peaktime_sim <- rep(NA,ncol(sim_res_all[[h]]))
for(i in 1:length(onset_sim)){
ILI_temp <- c(today$`% WEIGHTED ILI`,sim_res_all[[h]][,i])
above_bl <- (intersect(intersect(which(ILI_temp>baseline[h+1,13]),which(ILI_temp>baseline[h+1,13])+1),which(ILI_temp>baseline[h+1,13])+2)-2)
if (length(above_bl)!=0)  {onset_sim[i] <- min(above_bl)} else {onset_sim[i] <- NA}
peaksize_sim[i] <- max(ILI_temp)
peaktime_sim[i] <- which.max(ILI_temp)
}
jpeg(paste("Report",report,"_HHS",h,'.jpeg',sep=""),width=918,height=880)
layout(matrix(c(rep(1,8),2),nrow=9))
par(mar=c(1,5.1,2.4,2.1))
plot(0,ylim=c(0,10),xlim=c(1,52),ylab="wILI%",xlab="Week",xaxt='n',col='white',main=paste("HumNat ENVR - HHS Region",h),cex.main=2)
for(yr in 1997:2016) lines(get_flu_data("hhs",h,"ilinet",years=yr)$`% WEIGHTED ILI`,col="grey90",lwd=2)
points(y=today$`% WEIGHTED ILI`,x=1:(cur.week-40+1),pch=19,col="grey50")
lines(y=today$`% WEIGHTED ILI`,x=1:(cur.week-40+1),col="grey50",lwd=3)
lines(y=c(tail(today$`% WEIGHTED ILI`,1),rowMeans(sim_res_all[[h]])[1:4]),x=(cur.week-40+1):(cur.week-40+5),col="#FF69B4",lwd=2)
points(y=rowMeans(sim_res_all[[h]])[1:4],x= (cur.week-40+2):(cur.week-40+5), col=	"#FF69B4",pch=21,bg="white")
abline(v=(cur.week-40+3),lty=2)
abline(h=baseline[h+1,13],lty=2)
points(y=mean(peaksize_sim),x=mean(peaktime_sim),pch=19,col="#FF69B4")
lines(y=rep(mean(peaksize_sim),2),x=quantile(peaktime_sim,c(0.25,0.75)),col="#FF69B4",lty=2,lwd=2)
lines(y=c(mean(peaksize_sim)+0.15,mean(peaksize_sim)-0.15),x=rep(quantile(peaktime_sim,c(0.25)),2),col="#FF69B4",lwd=2)
lines(y=c(mean(peaksize_sim)+0.15,mean(peaksize_sim)-0.15),x=rep(quantile(peaktime_sim,c(0.75)),2),col="#FF69B4",lwd=2)
lines(x=rep(mean(peaktime_sim),2),y=quantile(peaksize_sim,c(0.25,0.75)),col="#FF69B4",lty=2,lwd=2)
lines(y=rep(quantile(peaksize_sim,c(0.25)),2),x=c(mean(peaktime_sim)+0.3,mean(peaktime_sim)-0.3),col="#FF69B4",lwd=2)
lines(y=rep(quantile(peaksize_sim,c(0.75)),2),x=c(mean(peaktime_sim)+0.3,mean(peaktime_sim)-0.3),col="#FF69B4",lwd=2)
par(mar=c(5.1,5.1,0,2.1))
plot(x=mean(onset_sim,na.rm=T),xlim=c(1,52),y=1,xaxt='n',yaxt="n",ylab="",xlab="Week",pch=19, col="#FF69B4")
axis(1,at=1:52,labels=c(40:52,1:39))
lines(x=quantile(onset_sim,na.rm=T,c(0.25,0.75)),y=rep(1,2),col="#FF69B4",lty=2,lwd=2)
lines(x=rep(quantile(onset_sim,na.rm=T,c(0.25)),2),y=c(1-0.1,1+0.1),col="#FF69B4",lwd=2)
lines(x=rep(quantile(onset_sim,na.rm=T,c(0.75)),2),y=c(1-0.1,1+0.1),col="#FF69B4",lwd=2)
mtext(expression("Onset\nWeek"),side=2,las=1,line=1,cex=0.75,padj=1)
dev.off()
}
---
### Current National Overview
The two panels below shows the relative wILI% of each HHS region using the most recently updated observation data and the nearest future forecast data. For instance, when wILI% data for week t becomes available, the top panel will be the national overall at time $t$, whereas the bottom pannel will be the national overall forecast for time t+1. Relative wILI% is calculated as:
```rwILI%_{h,t} = {wILI%_{h,t}}/{baseline_{h,s}}```
require(cdcfluview); require(maps); require(stringr)
setwd("~/Google Drive/Influenza/17-18_forecast/HumNat")
list.files()
baseline <- read.csv("wILI_Baseline.csv",stringsAsFactors = F)
setwd("~/Google Drive/Influenza/17-18_forecast/HumNat/RawResults")
list.files()
submission_date <- seq(as.Date("2017-11-06"),as.Date("2018-05-14"),7)
report=3
list.files()[grep(submission_date[report],list.files())][2]
load(list.files()[grep(submission_date[report],list.files())][2])
list.files()[grep(submission_date[report],list.files())][1]
load(list.files()[grep(submission_date[report],list.files())][1])
#plot 1
states <- map("state", plot = F)
unit_names <- states$names
unit_names <- gsub("\\:.*","",unit_names)
hhs <- cdcfluview::hhs_regions
unit_region <- data.frame(hhs_region = hhs$region_number[sapply(1:length(unit_names), function(y) which(toupper(unit_names[y])==toupper(hhs$state_or_territory)))])
col_gradient <- rbind(cbind(round(seq(0,1,length.out=101),2),colorRampPalette(c('blue', 'white'))(101))[1:100,],
cbind(round(seq(1,7,length.out=601),2),colorRampPalette(c('white', 'red'))(601)))
today <- get_flu_data("hhs",1:10,"ilinet",2017)
cur.week <- max(today$WEEK)
today <- round(today[today$WEEK==max(today$WEEK),5]/baseline[2:11,13],2)
tomo <- data.frame(round(sapply(1:10,function(h) mean(as.numeric(sim_res_all[[h]][1,])))/baseline[2:11,13],2))
unit_region["wILI_today"] <- sapply(1:nrow(unit_region), function(x) today[unit_region$hhs_region[x],1])
unit_region["col_today"] <- sapply(1:nrow(unit_region), function(x) col_gradient[which(as.numeric(col_gradient[,1])==unit_region$wILI_today[x]),2])
unit_region["wILI_tomo"] <-  sapply(1:nrow(unit_region), function(x) tomo[unit_region$hhs_region[x],1])
unit_region["col_tomo"] <- sapply(1:nrow(unit_region), function(x) col_gradient[which(as.numeric(col_gradient[,1])==unit_region$wILI_tomo[x]),2])
paste("~/Github/1718Flu/WP_plots/",paste("Report",report,sep=""),sep="")
setwd(paste("~/Github/1718Flu/WP_plots/",paste("Report",report,sep=""),sep=""))
today <- get_flu_data("national",,"ilinet",2017)
jpeg(paste("Report",report,"_national.jpeg",sep=""),width=918,height=880)
layout(matrix(c(rep(1,9),2),nrow=10))
par(mar=c(1,5.1,2.4,2.1))
plot(0,ylim=c(0,10),xlim=c(1,52),ylab="wILI%",xlab="Week",xaxt='n',col='white',main=paste("HumNat ENVR - National"),cex.main=2,cex.lab=2)
for(yr in 1997:2016) lines(get_flu_data("national",,"ilinet",years=yr)$`% WEIGHTED ILI`,col="grey90",lwd=2)
points(y=today$`% WEIGHTED ILI`,x=1:(cur.week-40+1),pch=19,col="grey50",)
lines(y=today$`% WEIGHTED ILI`,x=1:(cur.week-40+1),col="grey50",lwd=3)
lines(y=c(tail(today$`% WEIGHTED ILI`,1),rowMeans(sim_res_national)[1:4]),x=(cur.week-40+1):(cur.week-40+5),col="#FF69B4",lwd=2)
points(y=rowMeans(sim_res_national)[1:4],x= (cur.week-40+2):(cur.week-40+5), col=	"#FF69B4",pch=21,bg="white")
abline(v=(cur.week-40+3),lty=2)
abline(h=baseline[1,13],lty=2)
points(y=mean(peaksize),x=mean(peaktime),pch=19,col="#FF69B4")
lines(y=rep(mean(peaksize),2),x=quantile(peaktime,c(0.25,0.75)),col="#FF69B4",lty=2,lwd=2)
lines(y=c(mean(peaksize)+0.15,mean(peaksize)-0.15),x=rep(quantile(peaktime,c(0.25)),2),col="#FF69B4",lwd=2)
lines(y=c(mean(peaksize)+0.15,mean(peaksize)-0.15),x=rep(quantile(peaktime,c(0.75)),2),col="#FF69B4",lwd=2)
lines(x=rep(mean(peaktime),2),y=quantile(peaksize,c(0.25,0.75)),col="#FF69B4",lty=2,lwd=2)
lines(y=rep(quantile(peaksize,c(0.25)),2),x=c(mean(peaktime)+0.3,mean(peaktime)-0.3),col="#FF69B4",lwd=2)
lines(y=rep(quantile(peaksize,c(0.75)),2),x=c(mean(peaktime)+0.3,mean(peaktime)-0.3),col="#FF69B4",lwd=2)
par(mar=c(5.1,5.1,0,2.1))
plot(x=mean(onsettime,na.rm=T),xlim=c(1,52),y=1,xaxt='n',yaxt="n",ylab="",xlab="Week",pch=19, col="#FF69B4",cex.lab=2)
axis(1,at=1:52,labels=c(40:52,1:39))
lines(x=quantile(onsettime,na.rm=T,c(0.25,0.75)),y=rep(1,2),col="#FF69B4",lty=2,lwd=2)
lines(x=rep(quantile(onsettime,na.rm=T,c(0.25)),2),y=c(1-0.1,1+0.1),col="#FF69B4",lwd=2)
lines(x=rep(quantile(onsettime,na.rm=T,c(0.75)),2),y=c(1-0.1,1+0.1),col="#FF69B4",lwd=2)
mtext(expression("Onset\nWeek"),side=2,las=1,line=1,cex=0.75,padj=1)
dev.off()
setwd("~/Google Drive")
setwd("~/Google Drive/Influenza")
setwd("~/Google Drive/Influenza/17-18flu")
list.files()
setwd("~/Google Drive/Influenza/17-18_forecast/HumNat/html")
require(devtools);require(knitr);require(XMLRPC);require(RWordPress)
options(WordpressLogin = c(yangliubeijing = "0810Matt1M0"),
WordpressURL = "https://umnflu.wordpress.com/xmlrpc,php")
knit2wp("Report3.Rmd",title"Forecast 3: 2017-11-20 (ENVR)")
knit2wp("Report3.Rmd",title="Forecast 3: 2017-11-20 (ENVR)")
options(WordpressLogin = c(yangliubeijing = "0810Matt1M0"),
WordpressURL = "https://umnflu.wordpress.com/xmlrpc,php")
knit2wp("Report3.Rmd",title="Forecast 3: 2017-11-20 (ENVR)")
options(WordpressLogin = c(yangliubeijing = "0810Matt1M0"),
WordpressURL = "https://umnflu.wordpress.com/xmlrpc.php")
knit2wp("Report3.Rmd",title="Forecast 3: 2017-11-20 (ENVR)")
require(XML)
setwd("~/Documents/NSF")
years <- list.files()
nsf <- list()
#testing with NSF year of 1976
setwd("~/Documents/NSF")
load("nsf.RData")
setwd("~/Documents/NSF/2018")
all <- list.files()
current <- xmlInternalTreeParse(all[1], useInternalNodes = T)
node_list <- xmlToList(xmlInternalTreeParse(all[1]))
labels <- paste("//",gsub("\\.","/",substring(names(unlist(node_list)),7)),sep="")
target <- substring(names(unlist(node_list)),7) [c(1:5,7:8,10,13,19:21,25,28,30)]
y=27
path <- paste("~/Documents/NSF/",years[y],sep="")
setwd(path)
all <- list.files()
all
nsf[[y]] <- data.frame(matrix(NA,ncol=length(target),nrow=length(all)))
colnames(nsf[[y]]) <- target
for(i in 1:length(all)){
current <- xmlInternalTreeParse(all[i], useInternalNodes = T)
node_list <- xmlToList(xmlInternalTreeParse(all[i]))
labels <- paste("//",gsub("\\.","/",substring(names(unlist(node_list)),7)),sep="")
xml.res <- data.frame(cbind(sapply(1:length(labels),function(l) xpathSApply(current,labels[l],function(x) xmlSApply(x,xmlValue))),substring(names(unlist(node_list)),7)))
colnames(xml.res) <- c("Value","Label")
xml.res <- apply(xml.res,2,as.character)
for (c in 1:length(target)){
if (length(which(xml.res[,2]==colnames(nsf[[y]])[c]))==1) {
nsf[[y]][i,c] <- paste(xml.res[which(xml.res[,2]==colnames(nsf[[y]])[c]),1],collapse="|")
}
i
all[6205]
yr[y]
year[y]
years[y]
for(i in 6200:length(all)){
current <- xmlInternalTreeParse(all[i], useInternalNodes = T)
node_list <- xmlToList(xmlInternalTreeParse(all[i]))
labels <- paste("//",gsub("\\.","/",substring(names(unlist(node_list)),7)),sep="")
xml.res <- data.frame(cbind(sapply(1:length(labels),function(l) xpathSApply(current,labels[l],function(x) xmlSApply(x,xmlValue))),substring(names(unlist(node_list)),7)))
colnames(xml.res) <- c("Value","Label")
xml.res <- apply(xml.res,2,as.character)
for (c in 1:length(target)){
if (length(which(xml.res[,2]==colnames(nsf[[y]])[c]))==1) {
nsf[[y]][i,c] <- paste(xml.res[which(xml.res[,2]==colnames(nsf[[y]])[c]),1],collapse="|")
}
current <- xmlInternalTreeParse(all[i], useInternalNodes = T)
all[u]
all[i]
require(XML)
setwd("~/Documents/NSF")
years <- list.files()
nsf <- list()
#testing with NSF year of 1976
setwd("~/Documents/NSF")
load("nsf.RData")
setwd("~/Documents/NSF/2018")
all <- list.files()
current <- xmlInternalTreeParse(all[1], useInternalNodes = T)
node_list <- xmlToList(xmlInternalTreeParse(all[1]))
labels <- paste("//",gsub("\\.","/",substring(names(unlist(node_list)),7)),sep="")
target <- substring(names(unlist(node_list)),7) [c(1:5,7:8,10,13,19:21,25,28,30)]
path <- paste("~/Documents/NSF/",years[y],sep="")
setwd(path)
all <- list.files()
nsf[[y]] <- data.frame(matrix(NA,ncol=length(target),nrow=length(all)))
colnames(nsf[[y]]) <- target
i=6207
i=6205
current <- xmlInternalTreeParse(all[i], useInternalNodes = T)
all[i]
all[i+1]
require(XML)
setwd("~/Documents/NSF")
years <- list.files()
nsf <- list()
#testing with NSF year of 1976
setwd("~/Documents/NSF")
load("nsf.RData")
setwd("~/Documents/NSF/2018")
all <- list.files()
current <- xmlInternalTreeParse(all[1], useInternalNodes = T)
node_list <- xmlToList(xmlInternalTreeParse(all[1]))
labels <- paste("//",gsub("\\.","/",substring(names(unlist(node_list)),7)),sep="")
target <- substring(names(unlist(node_list)),7) [c(1:5,7:8,10,13,19:21,25,28,30)]
all[i]
y=27
path <- paste("~/Documents/NSF/",years[y],sep="")
setwd(path)
all <- list.files()
nsf[[y]] <- data.frame(matrix(NA,ncol=length(target),nrow=length(all)))
colnames(nsf[[y]]) <- target
all[6205]
nsf[[26]]
for (y in 26:43){
path <- paste("~/Documents/NSF/",years[y],sep="")
setwd(path)
all <- list.files()
nsf[[y]] <- data.frame(matrix(NA,ncol=length(target),nrow=length(all)))
colnames(nsf[[y]]) <- target
for(i in 1:length(all)){
current <- xmlInternalTreeParse(all[i], useInternalNodes = T)
node_list <- xmlToList(xmlInternalTreeParse(all[i]))
labels <- paste("//",gsub("\\.","/",substring(names(unlist(node_list)),7)),sep="")
xml.res <- data.frame(cbind(sapply(1:length(labels),function(l) xpathSApply(current,labels[l],function(x) xmlSApply(x,xmlValue))),substring(names(unlist(node_list)),7)))
colnames(xml.res) <- c("Value","Label")
xml.res <- apply(xml.res,2,as.character)
for (c in 1:length(target)){
if (length(which(xml.res[,2]==colnames(nsf[[y]])[c]))==1) {
nsf[[y]][i,c] <- paste(xml.res[which(xml.res[,2]==colnames(nsf[[y]])[c]),1],collapse="|")
}
setwd("~/Documents/NSF")
save(nsf,file="nsf.RData")
all[i]
years[y]
current <- xmlInternalTreeParse(all[i], useInternalNodes = T)
node_list <- xmlToList(xmlInternalTreeParse(all[i]))
labels <- paste("//",gsub("\\.","/",substring(names(unlist(node_list)),7)),sep="")
current <- xmlInternalTreeParse(all[i], useInternalNodes = T)
current <- xmlInternalTreeParse(all[i+1], useInternalNodes = T)
current <- xmlInternalTreeParse(all[i+2], useInternalNodes = T)
current <- xmlInternalTreeParse(all[i-], useInternalNodes = T)
current <- xmlInternalTreeParse(all[i-1], useInternalNodes = T)
i
i-1
xmlInternalTreeParse(all[i], useInternalNodes = T)
xmlInternalTreeParse(all[i-1], useInternalNodes = T)
xmlInternalTreeParse(all[1], useInternalNodes = T)
path <- paste("~/Documents/NSF/",years[y],sep="")
setwd(path)
all <- list.files()
nsf[[y]] <- data.frame(matrix(NA,ncol=length(target),nrow=length(all)))
colnames(nsf[[y]]) <- target
i=1
current <- xmlInternalTreeParse(all[i], useInternalNodes = T)
i=10040
current <- xmlInternalTreeParse(all[i], useInternalNodes = T)
for (y in 32:43){
path <- paste("~/Documents/NSF/",years[y],sep="")
setwd(path)
all <- list.files()
nsf[[y]] <- data.frame(matrix(NA,ncol=length(target),nrow=length(all)))
colnames(nsf[[y]]) <- target
for(i in 1:length(all)){
current <- xmlInternalTreeParse(all[i], useInternalNodes = T)
node_list <- xmlToList(xmlInternalTreeParse(all[i]))
labels <- paste("//",gsub("\\.","/",substring(names(unlist(node_list)),7)),sep="")
xml.res <- data.frame(cbind(sapply(1:length(labels),function(l) xpathSApply(current,labels[l],function(x) xmlSApply(x,xmlValue))),substring(names(unlist(node_list)),7)))
colnames(xml.res) <- c("Value","Label")
xml.res <- apply(xml.res,2,as.character)
for (c in 1:length(target)){
if (length(which(xml.res[,2]==colnames(nsf[[y]])[c]))==1) {
nsf[[y]][i,c] <- paste(xml.res[which(xml.res[,2]==colnames(nsf[[y]])[c]),1],collapse="|")
}
setwd("~/Documents/NSF")
save(nsf,file="nsf.RData")
nsf[[43]]
setwd("~/Documents/NSF")
load("nsf.RData")
nrow(nsf[[1]])
sapply(1:43, function(x) nrow(nsf[[x]]))
plot(sapply(1:43, function(x) nrow(nsf[[x]])))
nsf[[1]]$AwardAmount
as.numeric(nsf[[1]]$AwardAmount)
sum(as.numeric(nsf[[1]]$AwardAmount))
sum(as.numeric(nsf[[1]]$AwardAmount),na.rm=T)
sapply(1:43, function(x) sum(as.numeric(nsf[[x]]$AwardAmount),na.rm=T))
plot(sapply(1:43, function(x) sum(as.numeric(nsf[[x]]$AwardAmount),na.rm=T)))
require(cdcfluview); require(maps); require(stringr)
setwd("~/Google Drive/Influenza/17-18_forecast/HumNat")
list.files()
baseline <- read.csv("wILI_Baseline.csv",stringsAsFactors = F)
setwd("~/Google Drive/Influenza/17-18_forecast/HumNat/RawResults")
list.files()
submission_date <- seq(as.Date("2017-11-06"),as.Date("2018-05-14"),7)
report=4
list.files()[grep(submission_date[report],list.files())][2]
load(list.files()[grep(submission_date[report],list.files())][2])
list.files()[grep(submission_date[report],list.files())][1]
load(list.files()[grep(submission_date[report],list.files())][1])
#plot 1
states <- map("state", plot = F)
unit_names <- states$names
unit_names <- gsub("\\:.*","",unit_names)
hhs <- cdcfluview::hhs_regions
unit_region <- data.frame(hhs_region = hhs$region_number[sapply(1:length(unit_names), function(y) which(toupper(unit_names[y])==toupper(hhs$state_or_territory)))])
col_gradient <- rbind(cbind(round(seq(0,1,length.out=101),2),colorRampPalette(c('blue', 'white'))(101))[1:100,],
cbind(round(seq(1,7,length.out=601),2),colorRampPalette(c('white', 'red'))(601)))
today <- get_flu_data("hhs",1:10,"ilinet",2017)
cur.week <- max(today$WEEK)
today <- round(today[today$WEEK==max(today$WEEK),5]/baseline[2:11,13],2)
tomo <- data.frame(round(sapply(1:10,function(h) mean(as.numeric(sim_res_all[[h]][1,])))/baseline[2:11,13],2))
unit_region["wILI_today"] <- sapply(1:nrow(unit_region), function(x) today[unit_region$hhs_region[x],1])
unit_region["col_today"] <- sapply(1:nrow(unit_region), function(x) col_gradient[which(as.numeric(col_gradient[,1])==unit_region$wILI_today[x]),2])
unit_region["wILI_tomo"] <-  sapply(1:nrow(unit_region), function(x) tomo[unit_region$hhs_region[x],1])
unit_region["col_tomo"] <- sapply(1:nrow(unit_region), function(x) col_gradient[which(as.numeric(col_gradient[,1])==unit_region$wILI_tomo[x]),2])
paste("~/Github/1718Flu/WP_plots/",paste("Report",report,sep=""),sep="")
setwd(paste("~/Github/1718Flu/WP_plots/",paste("Report",report,sep=""),sep=""))
jpeg(paste("Report",report,"_tmap.jpeg",sep=""),width = 918, height = 600)
map("state",fill=T,col=unit_region$col_today,myborder = c(0.1,0.1))
legend('top', pch = 15, col = col_gradient[seq(0,700,50)+1,2], legend = as.numeric(col_gradient[seq(0,700,50)+1,1]), cex = 1, bty = 'n',title="Relative wILI%",horiz =T)
mtext(side=3,paste('Observation - Week', cur.week),cex=2)
dev.off()
jpeg(paste("Report",report,"_t+1map.jpeg",sep=""),width = 918, height = 600)
map("state",fill=T,col=unit_region$col_tomo,myborder = c(0.1,0.1))
legend('top', pch = 15, col = col_gradient[seq(0,700,50)+1,2], legend = as.numeric(col_gradient[seq(0,700,50)+1,1]), cex = 1, bty = 'n',title="Relative wILI%",horiz=T)
mtext(side=3,paste('Forecast - Week', cur.week+1),cex=2)
dev.off()
#plot2, HHS
#setwd("~/Google Drive/Influenza/17-18_forecast/HumNat/html")
#setwd("/Users/Yang/GitHub/1718flu")
paste("~/Github/1718Flu/WP_plots/",paste("Report",report,sep=""),sep="")
for(h in 1:10){
today <- get_flu_data("hhs",h,"ilinet",2017)
onset_sim <- peaksize_sim <- peaktime_sim <- rep(NA,ncol(sim_res_all[[h]]))
for(i in 1:length(onset_sim)){
ILI_temp <- c(today$`% WEIGHTED ILI`,sim_res_all[[h]][,i])
above_bl <- (intersect(intersect(which(ILI_temp>baseline[h+1,13]),which(ILI_temp>baseline[h+1,13])+1),which(ILI_temp>baseline[h+1,13])+2)-2)
if (length(above_bl)!=0)  {onset_sim[i] <- min(above_bl)} else {onset_sim[i] <- NA}
peaksize_sim[i] <- max(ILI_temp)
peaktime_sim[i] <- which.max(ILI_temp)
}
jpeg(paste("Report",report,"_HHS",h,'.jpeg',sep=""),width=918,height=880)
layout(matrix(c(rep(1,8),2),nrow=9))
par(mar=c(1,5.1,2.4,2.1))
plot(0,ylim=c(0,10),xlim=c(1,52),ylab="wILI%",xlab="Week",xaxt='n',col='white',main=paste("HumNat ENVR - HHS Region",h),cex.main=2)
for(yr in 1997:2016) lines(get_flu_data("hhs",h,"ilinet",years=yr)$`% WEIGHTED ILI`,col="grey90",lwd=2)
points(y=today$`% WEIGHTED ILI`,x=1:(cur.week-40+1),pch=19,col="grey50")
lines(y=today$`% WEIGHTED ILI`,x=1:(cur.week-40+1),col="grey50",lwd=3)
lines(y=c(tail(today$`% WEIGHTED ILI`,1),rowMeans(sim_res_all[[h]])[1:4]),x=(cur.week-40+1):(cur.week-40+5),col="#FF69B4",lwd=2)
points(y=rowMeans(sim_res_all[[h]])[1:4],x= (cur.week-40+2):(cur.week-40+5), col=	"#FF69B4",pch=21,bg="white")
abline(v=(cur.week-40+3),lty=2)
abline(h=baseline[h+1,13],lty=2)
points(y=mean(peaksize_sim),x=mean(peaktime_sim),pch=19,col="#FF69B4")
lines(y=rep(mean(peaksize_sim),2),x=quantile(peaktime_sim,c(0.25,0.75)),col="#FF69B4",lty=2,lwd=2)
lines(y=c(mean(peaksize_sim)+0.15,mean(peaksize_sim)-0.15),x=rep(quantile(peaktime_sim,c(0.25)),2),col="#FF69B4",lwd=2)
lines(y=c(mean(peaksize_sim)+0.15,mean(peaksize_sim)-0.15),x=rep(quantile(peaktime_sim,c(0.75)),2),col="#FF69B4",lwd=2)
lines(x=rep(mean(peaktime_sim),2),y=quantile(peaksize_sim,c(0.25,0.75)),col="#FF69B4",lty=2,lwd=2)
lines(y=rep(quantile(peaksize_sim,c(0.25)),2),x=c(mean(peaktime_sim)+0.3,mean(peaktime_sim)-0.3),col="#FF69B4",lwd=2)
lines(y=rep(quantile(peaksize_sim,c(0.75)),2),x=c(mean(peaktime_sim)+0.3,mean(peaktime_sim)-0.3),col="#FF69B4",lwd=2)
par(mar=c(5.1,5.1,0,2.1))
plot(x=mean(onset_sim,na.rm=T),xlim=c(1,52),y=1,xaxt='n',yaxt="n",ylab="",xlab="Week",pch=19, col="#FF69B4")
axis(1,at=1:52,labels=c(40:52,1:39))
lines(x=quantile(onset_sim,na.rm=T,c(0.25,0.75)),y=rep(1,2),col="#FF69B4",lty=2,lwd=2)
lines(x=rep(quantile(onset_sim,na.rm=T,c(0.25)),2),y=c(1-0.1,1+0.1),col="#FF69B4",lwd=2)
lines(x=rep(quantile(onset_sim,na.rm=T,c(0.75)),2),y=c(1-0.1,1+0.1),col="#FF69B4",lwd=2)
mtext(expression("Onset\nWeek"),side=2,las=1,line=1,cex=0.75,padj=1)
dev.off()
}
today <- get_flu_data("national",,"ilinet",2017)
jpeg(paste("Report",report,"_national.jpeg",sep=""),width=918,height=880)
layout(matrix(c(rep(1,9),2),nrow=10))
par(mar=c(1,5.1,2.4,2.1))
plot(0,ylim=c(0,10),xlim=c(1,52),ylab="wILI%",xlab="Week",xaxt='n',col='white',main=paste("HumNat ENVR - National"),cex.main=2,cex.lab=2)
for(yr in 1997:2016) lines(get_flu_data("national",,"ilinet",years=yr)$`% WEIGHTED ILI`,col="grey90",lwd=2)
points(y=today$`% WEIGHTED ILI`,x=1:(cur.week-40+1),pch=19,col="grey50",)
lines(y=today$`% WEIGHTED ILI`,x=1:(cur.week-40+1),col="grey50",lwd=3)
lines(y=c(tail(today$`% WEIGHTED ILI`,1),rowMeans(sim_res_national)[1:4]),x=(cur.week-40+1):(cur.week-40+5),col="#FF69B4",lwd=2)
points(y=rowMeans(sim_res_national)[1:4],x= (cur.week-40+2):(cur.week-40+5), col=	"#FF69B4",pch=21,bg="white")
abline(v=(cur.week-40+3),lty=2)
abline(h=baseline[1,13],lty=2)
points(y=mean(peaksize),x=mean(peaktime),pch=19,col="#FF69B4")
lines(y=rep(mean(peaksize),2),x=quantile(peaktime,c(0.25,0.75)),col="#FF69B4",lty=2,lwd=2)
lines(y=c(mean(peaksize)+0.15,mean(peaksize)-0.15),x=rep(quantile(peaktime,c(0.25)),2),col="#FF69B4",lwd=2)
lines(y=c(mean(peaksize)+0.15,mean(peaksize)-0.15),x=rep(quantile(peaktime,c(0.75)),2),col="#FF69B4",lwd=2)
lines(x=rep(mean(peaktime),2),y=quantile(peaksize,c(0.25,0.75)),col="#FF69B4",lty=2,lwd=2)
lines(y=rep(quantile(peaksize,c(0.25)),2),x=c(mean(peaktime)+0.3,mean(peaktime)-0.3),col="#FF69B4",lwd=2)
lines(y=rep(quantile(peaksize,c(0.75)),2),x=c(mean(peaktime)+0.3,mean(peaktime)-0.3),col="#FF69B4",lwd=2)
par(mar=c(5.1,5.1,0,2.1))
plot(x=mean(onsettime,na.rm=T),xlim=c(1,52),y=1,xaxt='n',yaxt="n",ylab="",xlab="Week",pch=19, col="#FF69B4",cex.lab=2)
axis(1,at=1:52,labels=c(40:52,1:39))
lines(x=quantile(onsettime,na.rm=T,c(0.25,0.75)),y=rep(1,2),col="#FF69B4",lty=2,lwd=2)
lines(x=rep(quantile(onsettime,na.rm=T,c(0.25)),2),y=c(1-0.1,1+0.1),col="#FF69B4",lwd=2)
lines(x=rep(quantile(onsettime,na.rm=T,c(0.75)),2),y=c(1-0.1,1+0.1),col="#FF69B4",lwd=2)
mtext(expression("Onset\nWeek"),side=2,las=1,line=1,cex=0.75,padj=1)
dev.off()
require(devtools); require(knitr)
require(RWordPress); require(XMLRPC)
options(WordpressLogin = c(yangliubeijing = "0810Matt1M0"),
WordpressURL = "https://umnflu.wordpress.com//xmlrpc.php")#alter with your wordpress site address
knit2wp("~/Google Drive/Influenza/17-18_forecast/HumNat/html/Report4.Rmd",title="Forecast 2: 2017-11-27 (ENVR)", categories=c("forecast","ENVR"))
log(30)/3
(log(30)/3)*1
(log(30)/3)*2
(log(30)/3)*3
-(0.2*log(0.2,base=2)+0.8*log(0.8,base = 2))
sample(1:6,1)
-2.3/100+1
setwd("~/Desktop")
read.csv("AH_ICD.csv")
read.csv("AH_ICD.csv", stringsAsFactors = F)
ICD = read.csv("AH_ICD.csv", stringsAsFactors = F)
ICD
ICD = read.csv("AH_ICD.csv", stringsAsFactors = F, sep=" ")
View(ICD)
ICD = read.csv("AH_ICD.csv", stringsAsFactors = F, sep=" ")[,c(1,5)]
View(ICD)
#hypertension without heart disease
HWHD <- data.frame(matrix(NA,ncol=2,nrow=4))
colnames(HWHD) <- c("ICD Version", "ICD Code")
HWHD[,1] <- 7:10
View(HWHD)
HWHD[1,2] <- c(1,2,3)
HWHT_8 <- c("400#","400.0", "400.2","400.3", 401, 403)
#hypertension without heart disease
HWHT_7 <- c(444:447)
#hypertension without heart disease
HWHT_7 <- as.character(c(444:447))
HWHT_10 <- c("I10", "I12","I12.0","I12.9")
#hypertension without heart disease
HWHT_7 <- as.character(c(444:447))
HWHT_8 <- c("400#","400.0", "400.2","400.3", 401, 403)
HWHT_9 <- as.character(c(401, 403, 405))
HWHT_10 <- c("I10", "I12","I12.0","I12.9")
#hypertension without heart disease
HWHT_7 <- as.character(c(444:447))
HWHT_8 <- c("400#","400.0", "400.2","400.3", 401, 403)
HWHT_9 <- as.character(c(401, 403, 405))
HWHT_10 <- c("I10#", "I12#","I12.0","I12.9")
setwd("~/Desktop")
ICD = read.csv("AH_ICD.csv", stringsAsFactors = F, sep=" ")[,c(1,5)]
which(ICD$icd_version==7)
ICD[which(ICD$icd_version==7),]
ICD[which(ICD$icd_version==8),]
ICD[which(ICD$icd_version==9),]
ICD[which(ICD$icd_version==10),]
grepl("#")
grepl("#",ICD[,2])
grepl(".",ICD[,2])
ICD
grepl(".",HWHT_10)
HWHT_10
grep(".",HWHT_10)
HWHT_10
grep("#",HWHT_10)
grep("\\.",HWHT_10)
grep("#",HWHT_10)
HWHT_10[grep("#",HWHT_10)]
which(ICD$cod2 %in% HWHT_10[grep("#",HWHT_10)])
which(ICD[,2]=="I10")
HWHT_10[grep("#",HWHT_10)]
gsub("#","",HWHT_10[grep("#",HWHT_10)])
which(ICD$cod2 %in% gsub("#","",HWHT_10[grep("#",HWHT_10)]))
ICD
ls()
rm(ls())
rm()
rm(ICD)
setwd("~/Desktop")
setwd("~/GitHub")
list.files()
setwd("~/GitHub/SPHStudentProject")
list.files()
setwd("~/GitHub/SPHStudentProject/AS_5m")
list.files()
AS <- list.files()
setwd("~/GitHub/SPHStudentProject/AH_5m")
AH <- list.files()
AH
gsub("AS","",AH)
gsub("AS_","",AH)
gsub(".csv","",gsub("AS_","",AH))
?substring
substring(gsub(".csv","",gsub("AS_","",AH)),1,7)
gsub(".csv","",gsub("AS_","",AH))
AH
setwd("~/GitHub/SPHStudentProject/AS_5m")
AS <- list.files()
setwd("~/GitHub/SPHStudentProject/AH_5m")
AH <- list.files()
gsub(".csv","",gsub("AS_","",AH))
gsub(".csv","",gsub("AH_","",AH))
substring(gsub(".csv","",gsub("AH_","",AH)),1,7)
plot(substring(gsub(".csv","",gsub("AH_","",AH)),1,7))
as.numeric(substring(gsub(".csv","",gsub("AH_","",AH)),1,7))
diff(as.numeric(substring(gsub(".csv","",gsub("AH_","",AH)),1,7)))
diff(diff(as.numeric(substring(gsub(".csv","",gsub("AH_","",AH)),1,7))))
diff(diff(as.numeric(substring(gsub(".csv","",gsub("AS_","",AS)),1,7))))
AH
AS
which(diff(diff(as.numeric(substring(gsub(".csv","",gsub("AS_","",AS)),1,7))))!=0)
AS[304]
AS[305]
AS[300:305]
(diff(as.numeric(substring(gsub(".csv","",gsub("AS_","",AS)),1,7))))
AS[305]
AS[300:310]
range(as.numeric(substring(gsub(".csv","",gsub("AH_","",AH)),1,7))))
range(as.numeric(substring(gsub(".csv","",gsub("AH_","",AH)),1,7)))
1000-931
AS
AS[300:305]
AS[300:310]
